{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, TFBertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'data/preprocessed_dataset.json'\n",
    "\n",
    "with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['context', 'question', 'answers'],\n",
      "        num_rows: 19\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['context', 'question', 'answers'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def createdataset(_data):\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    for i in _data['data']:\n",
    "        for j in i['paragraphs']:\n",
    "            context = j['context']\n",
    "            for k in j['qas']:\n",
    "                question = k['question']\n",
    "                for m in k['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append({'text': m['text'], 'answer_start': m['answer_start']})\n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "    'context': contexts,\n",
    "    'question': questions,\n",
    "    'answers': answers\n",
    "    })\n",
    "train = createdataset(dataset)\n",
    "\n",
    "train = train.train_test_split(test_size=0.2, seed=4444)\n",
    "\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 19/19 [00:00<00:00, 1119.42 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 551.27 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['context', 'question', 'answers', 'input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "        num_rows: 19\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['context', 'question', 'answers', 'input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['question'], examples['context'], truncation=True, padding='max_length', max_length = 512)\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i,answer in enumerate(examples['answers']):\n",
    "        start_position = answer['answer_start']\n",
    "        end_position = start_position + len(answer['text'])\n",
    "        start_positions.append(inputs.char_to_token(i, start_position))\n",
    "        end_positions.append(inputs.char_to_token(i, end_position -1))\n",
    "        if start_positions[-1] is None or end_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.cls_token_id\n",
    "            end_positions[-1] = tokenizer.cls_token_id\n",
    "    inputs.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    return inputs\n",
    "\n",
    "tokenized_train = train.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TFAutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_dataset = tokenized_train['train']\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'input_ids': train_dataset['input_ids'],\n",
    "        'attention_mask': train_dataset['attention_mask'],\n",
    "        'token_type_ids': train_dataset['token_type_ids']\n",
    "    },\n",
    "    {\n",
    "        'start_positions': train_dataset['start_positions'],\n",
    "        'end_positions': train_dataset['end_positions']\n",
    "    }\n",
    "))\n",
    "\n",
    "print(train_dataset)\n",
    "train_dataset = train_dataset.shuffle(1000).batch(4)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "              loss={'start_positions': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    'end_positions': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset, epochs=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
